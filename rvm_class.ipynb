{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import of the packages\n",
    "import numpy as np\n",
    "import scipy.optimize as optimize\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import scipy.stats as sts\n",
    "from collections import deque\n",
    "from numpy import linalg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParameterValueError(Exception):\n",
    "    \"\"\" Custom exception raised when a parameter has an invalid value.\"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RVM class\n",
    "class RVM_reg:\n",
    "    \"\"\" Relevance Vector Machine (RVM)\n",
    "    \n",
    "    Implementation of RVM for regression.\n",
    "    \n",
    "    Attributes:\n",
    "        kerType: A string of the type of the desired kernel.\n",
    "        rvmType: A string denoting the RVM type to be used. The string \"EM\" denotes\n",
    "            an EM-like algorithm will be used to estimate the hyperparameters, the\n",
    "            string \"DD\" denotes the direct differentiation approach while \"SSBL\"\n",
    "            denotes sequential sparse bayesian learning.\n",
    "        p: Integer value denoting the degree of the polynomial kernel.\n",
    "        sigma: Float value denoting the smoothing factor of the Gaussian kernel.\n",
    "        kappa: Float value denoting the scaling parameter of the sigmoid kernel.\n",
    "        delta: Float value denoting the translation parameter of the sigmoid kernel.\n",
    "        bTrained: boolean value which becomes true once the RVM has been trained.\n",
    "    \"\"\"\n",
    "\n",
    "    EPSILON_CONV = 1e-6\n",
    "    EPSILON_UF = 1e-30\n",
    "    TH_RV = 1e5\n",
    "    INFINITY = 1e20\n",
    "    maxEpochs = 5000\n",
    "    \n",
    "    def __init__(self, kerType = 'poly', rvmType = \"EM\", p = 1, sigma = 1, \n",
    "                 kappa = 1, delta = 1):\n",
    "        \"\"\" Initializes the RVM class (constructor).\n",
    "        \n",
    "            Raises:\n",
    "                ParameterValueError: An error occured because a parameter had an\n",
    "                    invalid value.  \n",
    "        \"\"\"\n",
    "        # Check if the kernel type chosen is valid\n",
    "        kerTypes = ['linear', 'poly', 'radial', 'sigmoid']\n",
    "        if kerType not in kerTypes:\n",
    "            raise ParameterValueError(\"ParameterValueError: The string \" + kerType + \\\n",
    "                                       \" does not denote a valid kernel type\")\n",
    "        # Check if the string denoting the rvmType has a valid value\n",
    "        if rvmType != 'EM' and rvmType != 'DD' and rvmType != \"SSBL\":\n",
    "            raise ParameterValueError('ParameterValueError: ' + rvmType, \\\n",
    "                                       \" is not a valid RVM type value. Enter 'EM', 'DD' or 'SSBL' as a value. \")\n",
    "       \n",
    "        self.kerType = kerType\n",
    "        self.rvmType = rvmType\n",
    "        self.p = p\n",
    "        self.sigma = sigma\n",
    "        self.kappa = kappa\n",
    "        self.delta = delta\n",
    "        self.bTrained = False\n",
    "\n",
    "    def kernel(self, x, y):\n",
    "        \"\"\" Kernel computation.\n",
    "        \n",
    "        It computes the kernel value based on the dot product\n",
    "        between two vectors.\n",
    "        \n",
    "        Args:\n",
    "            x: Input vector.\n",
    "            y: Other input vector.\n",
    "            \n",
    "        Returns:\n",
    "            The computed kernel value.\n",
    "        \"\"\"  \n",
    "        if self.kerType == \"linear\":\n",
    "            k = np.dot(x,y) + 1\n",
    "        elif self.kerType == \"poly\":\n",
    "            k = (np.dot(x,y) + 1) ** self.p\n",
    "        elif self.kerType == \"radial\":\n",
    "            k = math.exp(-(np.dot(x-y,x-y))/(2*self.sigma))\n",
    "        elif self.kerType == \"sigmoid\":\n",
    "            k = math.atanh(self.kappa * np.dot(x,y) - self.delta)\n",
    "\n",
    "        return k\n",
    "    \n",
    "    def getKernelMatrix(self, X, training = True):\n",
    "        \"\"\" Evaluates the kernel matrix K given a set of input samples (training).\n",
    "\n",
    "        Args:\n",
    "            X_tr: A NxM matrix with a M dimensional training input sample\n",
    "                in each row.\n",
    "\n",
    "        Returns:\n",
    "            An NxN Kernel matrix where N is the number of input samples.\n",
    "        \"\"\"\n",
    "        N = X.shape[0]\n",
    "        if training == True:\n",
    "            K = np.zeros((N, N))\n",
    "            for i in range(N):\n",
    "                for j in range(N):\n",
    "                    K[i,j] = self.kernel(X[i], X[j])\n",
    "        else:\n",
    "            N_sv = self.X_sv.shape[0]\n",
    "            K = np.zeros((N,N_sv))\n",
    "            for i in range(N):\n",
    "                for j in range(N_sv):\n",
    "                    K[i,j] = self.kernel(X[i], self.X_sv[j])\n",
    "        return K\n",
    "\n",
    "    def getGammaValues(self, alpha_values, Sigma):\n",
    "        \"\"\"Evaluates the gamma values.\n",
    "        \n",
    "        Args:\n",
    "            alpha_values: N-dimensional vector with the hyperparameters of\n",
    "                the marginal likelihood.\n",
    "            Sigma: NxN covariance matrix of the posterior\n",
    "\n",
    "        Returns: A N-dimensional vector with the gamma values where \n",
    "            gamma_values[i] = 1 - alpha_values[i] * Sigma[i][i]\n",
    "        \"\"\"\n",
    "        N = alpha_values.shape[0]\n",
    "        gamma_values = 1 - np.multiply(alpha_values, np.diag(Sigma))\n",
    "        return gamma_values\n",
    "        \n",
    "    def getAlphaValues(self, Sigma, mu, gamma_values):\n",
    "        \"\"\"Evaluates the alpha values.\n",
    "\n",
    "        Args:\n",
    "            Sigma: NxN covariance matrix of the posterior\n",
    "            mu: mean of the posterior\n",
    "            gamma_values: N-dimensional vector with gamma_values\n",
    "\n",
    "        Returns: A N-dimensional vector with the alpha_values\n",
    "        \"\"\"        \n",
    "        N = Sigma.shape[0]\n",
    "        alpha_values = np.zeros(N)\n",
    "        if self.rvmType == \"EM\":\n",
    "            cond_low = (np.diag(Sigma) + mu**2) < self.EPSILON_UF\n",
    "            cond_high = (np.diag(Sigma) + mu**2) > self.INFINITY\n",
    "            ncond = np.logical_and(np.logical_not(cond_low), np.logical_not(cond_high))\n",
    "            alpha_values[cond_low] = self.INFINITY\n",
    "            alpha_values[cond_high] = 0\n",
    "            alpha_values[ncond] = 1 / (np.diag(Sigma)[ncond] + mu[ncond]**2)\n",
    "        elif self.rvmType == \"DD\":\n",
    "            cond_low = (mu**2) < self.EPSILON_UF\n",
    "            cond_high = (mu**2) > self.INFINITY\n",
    "            ncond = np.logical_and(np.logical_not(cond_low), np.logical_not(cond_high))\n",
    "            alpha_values[cond_low] = self.INFINITY\n",
    "            alpha_values[cond_high] = 0\n",
    "            alpha_values[ncond] = gamma_values[ncond] / (mu[ncond]**2)\n",
    "        return alpha_values\n",
    "\n",
    "    \n",
    "    def train(self, X_tr, Y_tr):\n",
    "        \"\"\" RVM training method\n",
    "        \n",
    "        Applies an EM-like algorithm or direct differentiation to estimate the\n",
    "        optimal hyperparameters (alpha and sigma) needed to make predictions\n",
    "        using the marginal likelihood.\n",
    "        Alternatively, applies the Sequential Sparse Bayesian Algorithm to estimate\n",
    "        the optimal hyperparemeters (alpha and sigma) more efficiently.\n",
    "        \n",
    "        \n",
    "        Args:\n",
    "            X_tr: A matrix with a training input sample in each row.\n",
    "            Y_tr: A vector with the output values of each input sample\n",
    "                in X_tr.\n",
    "        \n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        # Get number of training data samples\n",
    "        N = X_tr.shape[0]\n",
    "        # Initialize the sigma squared value and the B matrix\n",
    "        sigma_squared = np.var(Y_tr) * 0.1\n",
    "        B = np.identity(N) / sigma_squared\n",
    "        # Calculate kernel matrix K and append a column with ones in the front \n",
    "        K = self.getKernelMatrix(X_tr)        \n",
    "        K = np.hstack((np.ones(N).reshape((N, 1)), K))\n",
    "        \n",
    "\n",
    "        if(self.rvmType == \"EM\" or self.rvmType ==\"DD\"):\n",
    "            ''' \n",
    "            Implementation based on the following paper Tipping, Michael. \n",
    "            \"Relevance vector machine.\" U.S. Patent No. 6,633,857. 14 Oct. 2003.\n",
    "            '''\n",
    "            # Initialize the alpha values (weight precision values) and the A matrix\n",
    "            alpha_values = np.ones(N + 1)\n",
    "            A = np.diag(alpha_values)\n",
    "            # Calculate Sigma and mu based on the initialized parameters\n",
    "            try:\n",
    "                Sigma = np.linalg.inv(K.T.dot(B).dot(K) + A)\n",
    "            except linalg.LinAlgError:\n",
    "                Sigma = np.linalg.pinv(K.T.dot(B).dot(K) + A)\n",
    "            mu = Sigma.dot(K.T).dot(B).dot(Y_tr)\n",
    "            # Calculate initial gamma values\n",
    "            gamma_values = self.getGammaValues(alpha_values, Sigma)\n",
    "\n",
    "            # Approximate optimal hyperparameter values iteratively\n",
    "            for epoch in range(self.maxEpochs):\n",
    "                # Evaluate alpha values\n",
    "                next_alpha_values = self.getAlphaValues(Sigma, mu, gamma_values)\n",
    "                # Evaluate sigma value\n",
    "                next_sigma_squared = (np.linalg.norm(Y_tr - K.dot(mu)) ** 2) / (N - np.sum(gamma_values))\n",
    "                # Check if algorithm has converged (variation of alpha and sigma)\n",
    "                if (np.sum(np.absolute(next_alpha_values - alpha_values)) < self.EPSILON_CONV and\n",
    "                    abs(next_sigma_squared - sigma_squared) < self.EPSILON_CONV):\n",
    "                        break\n",
    "                # If algorithm has not converged, update all the variables\n",
    "                alpha_values = next_alpha_values\n",
    "                sigma_squared = next_sigma_squared\n",
    "                A = np.diag(alpha_values)\n",
    "                B = np.identity(N) / sigma_squared\n",
    "                try:\n",
    "                    Sigma = np.linalg.inv(K.T.dot(B).dot(K) + A)\n",
    "                except linalg.LinAlgError:\n",
    "                    Sigma = np.linalg.pinv(K.T.dot(B).dot(K) + A) \n",
    "                mu = Sigma.dot(K.T).dot(B).dot(Y_tr)\n",
    "                gamma_values = self.getGammaValues(alpha_values, Sigma)\n",
    "                \n",
    "            # We store the support vectors and other important variables\n",
    "            cond_sv = alpha_values < self.TH_RV\n",
    "            self.X_sv = X_tr[cond_sv[1:N+1]]\n",
    "            self.Y_sv = Y_tr[cond_sv[1:N+1]]\n",
    "            self.mu = mu[cond_sv]\n",
    "            self.Sigma = Sigma[cond_sv][:,cond_sv]\n",
    "            self.sigma_squared = sigma_squared\n",
    "            \n",
    "        elif self.rvmType == \"SSBL\":     \n",
    "            \"\"\"\n",
    "            Implementation based on the following paper: Tipping, M.E. and Faul, A.C., 2003, January.\n",
    "            Fast marginal likelihood maximisation for sparse Bayesian models. In AISTATS.\n",
    "            \"\"\"\n",
    "            \n",
    "            # 2. Initialize one alpha value and set all the others to infinity.\n",
    "            alpha_values = np.zeros(N + 1) + self.INFINITY\n",
    "            basis_column = K[:,0]\n",
    "            phi_norm = np.linalg.norm(basis_column)\n",
    "            alpha_values[0] = phi_norm **2 / ((np.linalg.norm(basis_column.dot(Y_tr)) ** 2) \\\n",
    "                                          / (phi_norm ** 2) - sigma_squared)\n",
    "            included_cond = np.zeros(N + 1, dtype=bool)\n",
    "            included_cond[0] = True\n",
    "            \n",
    "            # 3. Initialize Sigma and mu\n",
    "            A = np.zeros(1) + alpha_values[0]\n",
    "            basis_column = basis_column.reshape((N, 1)) # Reshape so that it can be transposed\n",
    "            Sigma = 1 / (basis_column.T.dot(B).dot(basis_column) + A)\n",
    "            mu = Sigma.dot(basis_column.T).dot(B).dot(Y_tr)\n",
    "\n",
    "            # 3. Initialize q and s for all bases\n",
    "            q = np.zeros(N + 1)\n",
    "            Q = np.zeros(N + 1)\n",
    "            s = np.zeros(N + 1)\n",
    "            S = np.zeros(N + 1)\n",
    "            Phi = basis_column\n",
    "            for i in range(N + 1):\n",
    "                basis = K[:, i]\n",
    "                tmp_1 = basis.T.dot(B)\n",
    "                tmp_2 = tmp_1.dot(Phi).dot(Sigma).dot(Phi.T).dot(B)\n",
    "                Q[i] = tmp_1.dot(Y_tr) - tmp_2.dot(Y_tr)\n",
    "                S[i] = tmp_1.dot(basis) - tmp_2.dot(basis)\n",
    "            denom = (alpha_values - S)\n",
    "            s = (alpha_values * S) / denom\n",
    "            q = (alpha_values * Q) / denom\n",
    "            \n",
    "            # Create queue with indices to select candidates for update\n",
    "            queue = deque([i for i in range(N + 1)])\n",
    "            # Start updating the model iteratively\n",
    "            for epoch in range(self.maxEpochs):\n",
    "                # 4. Pick a candidate basis vector from the start of the queue and put it at the end\n",
    "                basis_idx = queue.popleft()\n",
    "                queue.append(basis_idx)\n",
    "                \n",
    "                # 5. Compute theta\n",
    "                theta = q ** 2 - s\n",
    "                \n",
    "                next_alpha_values = np.copy(alpha_values)\n",
    "                next_included_cond = np.copy(included_cond)\n",
    "                if theta[basis_idx] > 0 and alpha_values[basis_idx] < self.INFINITY:\n",
    "                    # 6. Re-estimate alpha\n",
    "                    next_alpha_values[basis_idx] = s[basis_idx] ** 2 / (q[basis_idx] ** 2 - s[basis_idx])\n",
    "                    pass\n",
    "                elif theta[basis_idx] > 0 and alpha_values[basis_idx] >= self.INFINITY:\n",
    "                    # 7. Add basis function to the model with updated alpha\n",
    "                    next_alpha_values[basis_idx] = s[basis_idx] ** 2 / (q[basis_idx] ** 2 - s[basis_idx])\n",
    "                    next_included_cond[basis_idx] = True\n",
    "                elif theta[basis_idx] <= 0 and alpha_values[basis_idx] < self.INFINITY:\n",
    "                    # 8. Delete theta basis function from model and set alpha to infinity\n",
    "                    next_alpha_values[basis_idx] = self.INFINITY\n",
    "                    next_included_cond[basis_idx] = False\n",
    "                    \n",
    "                # 9. Estimate noise level\n",
    "                gamma_values = 1 - np.multiply(alpha_values[included_cond], np.diag(Sigma))\n",
    "                next_sigma_squared = (np.linalg.norm(Y_tr - Phi.dot(mu)) ** 2) / (N - np.sum(gamma_values))\n",
    "                \n",
    "                # 11. Check for convergence\n",
    "                # Check if algorithm has converged (variation of alpha and sigma)\n",
    "                not_included_cond = np.logical_not(included_cond)\n",
    "                if (np.sum(np.absolute(next_alpha_values[included_cond] - alpha_values[included_cond])) \\\n",
    "                            < self.EPSILON_CONV) and all(th <= 0 for th in theta[not_included_cond]):              \n",
    "                        break\n",
    "                \n",
    "                # 10. Recompute/update  Sigma and mu as well as s and q\n",
    "                alpha_values = next_alpha_values\n",
    "                sigma_squared = next_sigma_squared\n",
    "                included_cond = next_included_cond\n",
    "                A = np.diag(alpha_values[included_cond])\n",
    "                B = np.identity(N) / sigma_squared\n",
    "                Phi = K[:, included_cond]\n",
    "                # Compute Sigma\n",
    "                tmp = Phi.T.dot(B).dot(Phi) + A\n",
    "                if(tmp.shape[0] == 1):\n",
    "                    Sigma = 1 / tmp\n",
    "                else:\n",
    "                    try:\n",
    "                        Sigma = np.linalg.inv(tmp)\n",
    "                    except linalg.LinAlgError:\n",
    "                        Sigma = np.linalg.pinv(tmp)\n",
    "                    \n",
    "                # Compute mu\n",
    "                mu = Sigma.dot(Phi.T).dot(B).dot(Y_tr)\n",
    "                # Update s and q\n",
    "                for i in range(N + 1):\n",
    "                    basis = K[:, i]\n",
    "                    tmp_1 = basis.T.dot(B)\n",
    "                    tmp_2 = tmp_1.dot(Phi).dot(Sigma).dot(Phi.T).dot(B)\n",
    "                    Q[i] = tmp_1.dot(Y_tr) - tmp_2.dot(Y_tr)\n",
    "                    S[i] = tmp_1.dot(basis) - tmp_2.dot(basis)\n",
    "                denom = (alpha_values - S)\n",
    "                s = (alpha_values * S) / denom\n",
    "                q = (alpha_values * Q) / denom\n",
    "            ##print(epoch)\n",
    "            # We store the relevance vectors and other important variables\n",
    "            self.X_sv = X_tr[included_cond[1:N+1]]\n",
    "            self.Y_sv = Y_tr[included_cond[1:N+1]]\n",
    "            self.mu = mu\n",
    "            self.Sigma = Sigma\n",
    "            self.sigma_squared = sigma_squared\n",
    "            \n",
    "            \n",
    "        self.bTrained = True\n",
    "    \n",
    "    def pred(self, X):\n",
    "        \"\"\"Predicts the classes for a number of input data\n",
    "        \n",
    "        Args:\n",
    "            X: matrix with input data where each row represents a sample.\n",
    "            \n",
    "        Returns:\n",
    "            y: A tuple with vector with the predicted class for each input sample\n",
    "                and the error variance.\n",
    "            \n",
    "        Raises:\n",
    "            UntrainedModelError: Error that occurs when this function is called\n",
    "                before calling the 'train' function.\n",
    "        \"\"\"\n",
    "        \n",
    "        if self.bTrained == False:\n",
    "            raise UntrainedModelError(\"UntrainedModelError: The SVM model has not been trained.\")\n",
    "        \n",
    "        N = X.shape[0]\n",
    "        K = self.getKernelMatrix(X, training = False)  \n",
    "        N_sv = np.shape(self.X_sv)[0]\n",
    "        if np.shape(self.mu)[0] != N_sv:\n",
    "            K = np.hstack((np.ones(N).reshape((N, 1)), K))\n",
    "        y = K.dot(self.mu)\n",
    "        err_var = self.sigma_squared + K.dot(self.Sigma).dot(K.T)\n",
    "        return y, np.sqrt(np.diag(err_var))\n",
    "\n",
    "    def getSV(self):\n",
    "        return self.X_sv, self.Y_sv\n",
    "               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
