{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random as rnd\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import scipy.stats as sts\n",
    "import copy\n",
    "from decimal import *\n",
    "from sklearn.datasets import load_iris\n",
    "from scipy.optimize import minimize\n",
    "from scipy.special import expit\n",
    "from sklearn.base import BaseEstimator, RegressorMixin, ClassifierMixin\n",
    "from sklearn.metrics.pairwise import (linear_kernel, rbf_kernel, polynomial_kernel, sigmoid_kernel, laplacian_kernel)\n",
    "from sklearn.multiclass import OneVsOneClassifier\n",
    "from sklearn.utils.validation import check_X_y\n",
    "import data_gen\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotData(x, y):\n",
    "    # Plot the data\n",
    "    classA = x[y == 1]\n",
    "    classB = x[y == 0]\n",
    "    plt.xlabel('X Axis (No units)')\n",
    "    plt.ylabel('Y Axis (No units)')\n",
    "    plt.plot(classA[:,0], classA[:,1], 'bo')\n",
    "    plt.plot(classB[:,0], classB[:,1], 'ro')\n",
    "    plt.legend([\"Class A\",\"Class B\"])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_tr: feature vectors of training dataset\n",
    "# t_tr: labels of training dataset\n",
    "# x_tst: grid of feature vectors for drawing classes' areas\n",
    "# t_tst: results of classification of the grid\n",
    "# x_sv: support vectors\n",
    "def plotClassification(x_tr, t_tr, x_tst, t_tst, x_sv):   \n",
    "    #First we plot the training points\n",
    "    classA_tr = x_tr[t_tr == 1]\n",
    "    classB_tr = x_tr[t_tr == 0]\n",
    "    \n",
    "    plt.plot(classA_tr[:,0], classA_tr[:,1], 'bo')\n",
    "    plt.plot(classB_tr[:,0], classB_tr[:,1], 'ro')\n",
    "      \n",
    "    #Then, we plot the support vectors\n",
    "    plt.plot(x_sv[:,0], x_sv[:,1] , 'go', markersize = 15, mfc = \"None\") \n",
    "    \n",
    "    plt.legend([\"Class A\",\"Class B\", \"Support Vectors\"])\n",
    "    \n",
    "    #Finally, we plot the test results\n",
    "    classA_tst = x_tst[t_tst == 1]\n",
    "    classB_tst = x_tst[t_tst == 0]\n",
    "    \n",
    "    plt.xlabel('X Axis (No units)')\n",
    "    plt.ylabel('Y Axis (No units)')\n",
    "\n",
    "    plt.plot(classA_tst[:,0], classA_tst[:,1], 'bs', alpha=0.05)\n",
    "    plt.plot(classB_tst[:,0], classB_tst[:,1], 'rs', alpha=0.05)\n",
    "    #plt.savefig('test.pdf', bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kernel(X1, X2, kernelType):\n",
    "    if kernelType == \"radial\":\n",
    "        kernelParams = [7.6]\n",
    "        return laplacian_kernel(X1, X2, kernelParams[0])\n",
    "    elif kernelType == \"linear\":\n",
    "        return linear_kernel(X1,X2)\n",
    "    elif kernelType == \"polynomial\":\n",
    "        degree = 2\n",
    "        return polynomial_kernel(X1,X2,degree)\n",
    "    elif kernelType == \"sigmoid\":\n",
    "        return sigmoid_kernel(X1,X2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = load_iris()\n",
    "#data_check, targ_check = check_X_y(iris.data, iris.target)\n",
    "X = []\n",
    "t = []\n",
    "\n",
    "for i in range(len(iris.data)):\n",
    "    if iris.target[i] == 1 or iris.target[i] == 2:\n",
    "        X.append(iris.data[i][:-2])\n",
    "        t.append((iris.target[i]-1)) # TODO: Identify unique labels, change to 0/1-format\n",
    "X = np.array(X)\n",
    "t = np.array(t)\n",
    "\n",
    "h = 55\n",
    "x_0_min = np.min(X[:,0])\n",
    "x_0_max = np.max(X[:,0])\n",
    "x_1_min = np.min(X[:,1])\n",
    "x_1_max = np.max(X[:,1])\n",
    "xx0, xx1 = np.meshgrid(np.linspace(x_0_min, x_0_max, num=h),\n",
    "                         np.linspace(x_1_min, x_1_max, num=h))\n",
    "X_tst = np.vstack((xx0.reshape(1,-1),xx1.reshape(1,-1))).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetchDataset(dataset):\n",
    "    return data_gen.genData(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function filters out infinite alpha values and the columns of the design\n",
    "# matrix that correspond to them\n",
    "def filterOutInf(alphaVec,designMatrix,threshHold):\n",
    "    nonInfIdx = np.where(alphaVec < threshHold)[0]\n",
    "    filteredAlphaVec = alphaVec[nonInfIdx]\n",
    "    filteredDesignMatrix = designMatrix[:,nonInfIdx]\n",
    "    return filteredAlphaVec, filteredDesignMatrix\n",
    "\n",
    "def sigmoidFn(y):\n",
    "    return 1/(1+np.exp(-y))\n",
    "\n",
    "def getqiAndsi(OGPhi,Phi,Phi_t, alphaVec,t,w,Sigma,threshold,basisInds,index):\n",
    "    phiI = OGPhi[:,index].reshape((-1,1))\n",
    "    phiI_t =phiI.T\n",
    "    alphaI = alphaVec[index]\n",
    "    y = np.array([np.dot(w.T,xn[basisInds]) for xn in OGPhi]).reshape((-1,1))\n",
    "    t = t.reshape((-1,1))\n",
    "    betas = np.array([sigmoidFn(yn)*(1-sigmoidFn(yn)) for yn in y]).flatten()\n",
    "    B = np.diag(betas)\n",
    "    if B.ndim == 1:\n",
    "        B = B.reshape((-1,1))\n",
    "    tEst = np.dot(Phi,w) + np.linalg.inv(B).dot((np.subtract(t,y)))\n",
    "\n",
    "    Qi = phiI_t.dot(B).dot(tEst)\n",
    "    Qi -= phiI_t.dot(B).dot(Phi).dot(Sigma).dot(Phi_t).dot(B).dot(tEst)\n",
    "\n",
    "    Si = phiI_t.dot(B).dot(phiI)\n",
    "    Si -= phiI_t.dot(B).dot(Phi).dot(Sigma).dot(Phi_t).dot(B).dot(phiI)\n",
    "\n",
    "    if alphaI >= threshold:\n",
    "        qi = Qi\n",
    "        si = Si\n",
    "    else:\n",
    "        qi = alphaI*Qi/(alphaI-Si)\n",
    "        si = alphaI*Si/(alphaI-Si)\n",
    "\n",
    "    return qi, si\n",
    "\n",
    "def getC(alphaVec,designMatrix,designMatrix_t,postMean):\n",
    "    y = expit(np.dot(designMatrix, postMean))\n",
    "    B = np.diag(y * (1 - y))\n",
    "    A = np.diag(alphaVec)\n",
    "    return B + np.dot(np.dot(designMatrix,A),designMatrix_t)\n",
    "    \n",
    "def updateAlphaValue(designMatrix,alphaVec,postMean,t,postCov,threshHold, index):\n",
    "    C = getC(alphaVec,designMatrix,postMean)\n",
    "    qi, si = getqiAndsi(designMatrix, alphaVec, C, t,postCov, threshHold,index)\n",
    "    if qi**2 > si:\n",
    "        return si**2/(qi**2-si)\n",
    "    else:\n",
    "        return math.inf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X: training feature vectors\n",
    "# t: training labels\n",
    "# kernel: function of the form \"kernel(X1, X2, kernel_params)\"\n",
    "# kern_params: params for the passed kernel function\n",
    "# returns: \n",
    "def RVM_C(X, t, kernelType,update_fn = \"DD\", max_iter_num = 1000, opt_max_iter = 100, tolerance = 1e-7, alpha_threshold = 1e7):\n",
    "\n",
    "    # Gamma prior variables:\n",
    "    a = 0\n",
    "    b = 0\n",
    "    c = 0\n",
    "    d = 0 # These are all set to 0 since we, just like Tipping, assume uniform priors for the hyperparams\n",
    "    \n",
    "    N = len(t)\n",
    "    Phi = kernel(X, X, kernelType) #rbf_kernel(X, X, kern_param)\n",
    "    Phi = np.append(Phi, np.ones((N, 1)), axis = 1)\n",
    "    Phi_t = Phi.T\n",
    "    opt_options = { 'maxiter': opt_max_iter }\n",
    "    biasFlag = True\n",
    "    numLessTol = 0\n",
    "\n",
    "    numRelevant = N+1\n",
    "    alphas = np.full(numRelevant, 1e-6)\n",
    "    w_mode = np.zeros(numRelevant)\n",
    "\n",
    "    alphas_res = None\n",
    "    Phi_res = None\n",
    "    w_res = None\n",
    "    X_res = copy.deepcopy(X)\n",
    "\n",
    "    for iterNum in range(max_iter_num):\n",
    "        opt_res = minimize(fun=getLogPosterior, x0=w_mode, hess=getHessian, method='Newton-CG', \\\n",
    "                           args=(alphas, Phi, Phi_t, t), jac=True, options=opt_options)\n",
    "\n",
    "        w_mode = opt_res.x\n",
    "        Sigma = np.linalg.inv(getHessian(w_mode, alphas, Phi, Phi_t, t))\n",
    "\n",
    "        # Prune basis functions\n",
    "\n",
    "        basisInds = []\n",
    "        sumDiff = 0\n",
    "        for i in range(numRelevant):\n",
    "            alpha_prev = alphas[i]\n",
    "            \n",
    "            if update_fn == \"DD\":\n",
    "                alphas[i] = (1 - Sigma[i][i]*alpha_prev)/(w_mode[i]*w_mode[i])\n",
    "            elif update_fn == \"EM\":\n",
    "                weightEst = Sigma[i,i] + w_mode[i]**2\n",
    "                alphas[i] = (1+2*a)/(weightEst+2*b)\n",
    "                \n",
    "            if (alphas[i] < alpha_threshold):\n",
    "                basisInds.append(i)\n",
    "                sumDiff += np.fabs(alphas[i]-alpha_prev)\n",
    "\n",
    "        if (sumDiff < tolerance):\n",
    "            numLessTol += 1\n",
    "        if (numLessTol > 3):\n",
    "            # Save results\n",
    "            \n",
    "            #print(alphas) ###\n",
    "\n",
    "            numRelevant = len(basisInds)\n",
    "\n",
    "            alphas_res = np.zeros(numRelevant)\n",
    "            Phi_res = np.zeros((N, numRelevant))\n",
    "            w_res = np.zeros(numRelevant)\n",
    "            X_new = np.zeros((numRelevant, len(X[0]-1)))\n",
    "\n",
    "            for i in range(numRelevant):\n",
    "                alphas_res[i] = alphas[basisInds[i]]\n",
    "                w_res[i] = w_mode[basisInds[i]]\n",
    "                for j in range(N):\n",
    "                    Phi_res[j][i] = Phi[j][basisInds[i]]\n",
    "                if (basisInds[i] != len(X_res)):\n",
    "                    for j in range(len(X[0])):\n",
    "                        X_new[i][j] = X_res[basisInds[i]][j]\n",
    "\n",
    "            X_res = X_new\n",
    "\n",
    "            break\n",
    "\n",
    "        if (len(basisInds) == 0):\n",
    "            basisInds.append(0)\n",
    "            # First iteration\n",
    "            if (biasFlag):\n",
    "                basisInds.append(len(alphas)-1)\n",
    "\n",
    "        if (biasFlag):\n",
    "            if not (alphas[-1] < alpha_threshold):\n",
    "                biasFlag = False\n",
    "\n",
    "        numRelevant = len(basisInds)\n",
    "\n",
    "        alphas_new = np.zeros(numRelevant)\n",
    "        Phi_new = np.zeros((N, numRelevant))\n",
    "        w_mode_new = np.zeros(numRelevant)\n",
    "        X_new = np.zeros((numRelevant, len(X[0])))\n",
    "\n",
    "        for i in range(numRelevant):\n",
    "            alphas_new[i] = alphas[basisInds[i]]\n",
    "            w_mode_new[i] = w_mode[basisInds[i]]\n",
    "            for j in range(N):\n",
    "                Phi_new[j][i] = Phi[j][basisInds[i]]\n",
    "            if (basisInds[i] != len(X_res)):\n",
    "                for j in range(len(X[0])):\n",
    "                    X_new[i][j] = X_res[basisInds[i]][j]\n",
    "\n",
    "        alphas = alphas_new\n",
    "        w_mode = w_mode_new\n",
    "        Phi = Phi_new\n",
    "        Phi_t = Phi.T\n",
    "        X_res = X_new\n",
    "        \n",
    "    # X_res: features of the support vectors\n",
    "    # alphas_res: support vectors alphas\n",
    "    # w_res: optimal parameters' values\n",
    "    # Phi_res: Phi's of the support vectors\n",
    "    return X_res, alphas_res, w_res, Phi_res\n",
    "\n",
    "def sparse_RVM_C(X, t, kernelType,update_fn = \"DD\", max_iter_num = 10000, opt_max_iter = 100, tolerance = 1e-7, alpha_threshold = 1e5):\n",
    "\n",
    "    N = len(t)\n",
    "    Phi = kernel(X, X, kernelType) #rbf_kernel(X, X, kern_param)\n",
    "    Phi = np.append(Phi, np.ones((N, 1)), axis = 1)\n",
    "    Phi_t = Phi.T\n",
    "    opt_options = { 'maxiter': opt_max_iter }\n",
    "    biasFlag = True\n",
    "    numLessTol = 0\n",
    "\n",
    "    numRelevant = 1\n",
    "    alphas = np.full(N+1, float('inf'))\n",
    "    alphas[0] = 1e-6\n",
    "    \n",
    "\n",
    "    alphas_res = None\n",
    "    Phi_res = None\n",
    "    w_res = None\n",
    "    X_res = copy.deepcopy(X)\n",
    "    usedPhi = Phi[:,0].reshape((-1,1))\n",
    "    basisInds =[0]\n",
    "    w_mode = np.zeros(len(alphas[basisInds]))\n",
    "    printStuff = False\n",
    "\n",
    "    for iterNum in range(max_iter_num):\n",
    "        \n",
    "        if printStuff:\n",
    "            print(\"New iteration\")\n",
    "            print(\"Used Phi:\")\n",
    "            print(usedPhi.shape)\n",
    "            print(\"w_mode:\")\n",
    "            print(w_mode.shape)\n",
    "            print(\"alphas:\")\n",
    "            print(alphas.shape)\n",
    "            print(\"basisInds:\")\n",
    "            print(basisInds)\n",
    "        w_mode = np.zeros(len(alphas[basisInds]))\n",
    "        usedPhi = Phi[:,basisInds]\n",
    "        opt_res = minimize(fun=getLogPosterior, x0=w_mode.reshape((-1,1)), hess=getHessian, method='Newton-CG', \\\n",
    "                           args=(alphas[basisInds], usedPhi, usedPhi.T, t), jac=True, options=opt_options)\n",
    "\n",
    "        w_mode = opt_res.x\n",
    "        Sigma = np.linalg.inv(getHessian(w_mode, alphas[basisInds], usedPhi, usedPhi.T, t))\n",
    "        sVec = []\n",
    "        qVec = []\n",
    "        \n",
    "        usedPhi = Phi[:,basisInds]\n",
    "        usedPhi_t = usedPhi.T\n",
    "        for i in range(len(alphas)):\n",
    "            qi, si = getqiAndsi(Phi,usedPhi,usedPhi_t, alphas,t,w_mode.reshape((-1,1)),Sigma,alpha_threshold,basisInds,i)\n",
    "            qVec.append(qi)\n",
    "            sVec.append(si)\n",
    "        \n",
    "        # Prune basis functions\n",
    "\n",
    "        sumDiff = 0\n",
    "        for i in range(len(alphas)):\n",
    "            alpha_prev = alphas[i]\n",
    "            \n",
    "            if update_fn == \"DD\":\n",
    "                if qVec[i]**2 > sVec[i]:\n",
    "                    if alphas[i] > alpha_threshold:\n",
    "                        basisInds.append(i)\n",
    "                        numRelevant += 1\n",
    "                    \n",
    "                    alphas[i] = sVec[i]**2/(qVec[i]**2-sVec[i])\n",
    "                elif qVec[i]**2 <= sVec[i] and i in basisInds:\n",
    "                    alphas[i] = float('inf')\n",
    "                    basisInds.remove(i)\n",
    "                    numRelevant -= 1\n",
    "                    \n",
    "            elif update_fn == \"EM\":\n",
    "                weightEst = Sigma[i,i] + w_mode[i]**2\n",
    "                alphas[i] = (1+2*a)/(weightEst+2*b)\n",
    "                \n",
    "            if (alphas[i] < alpha_threshold):\n",
    "                #basisInds.append(i)\n",
    "                sumDiff += np.fabs(alphas[i]-alpha_prev)\n",
    "\n",
    "        if (sumDiff < tolerance):\n",
    "            numLessTol += 1\n",
    "        if (numLessTol > 3):\n",
    "            # Save results\n",
    "\n",
    "            break\n",
    "\n",
    "        if (len(basisInds) == 0):\n",
    "            basisInds.append(0)\n",
    "            # First iteration\n",
    "            if (biasFlag):\n",
    "                basisInds.append(len(alphas)-1)\n",
    "\n",
    "        if (biasFlag):\n",
    "            if not (alphas[-1] < alpha_threshold):\n",
    "                biasFlag = False\n",
    "                \n",
    "    \n",
    "    alphas_res = alphas[basisInds]\n",
    "    X_res = X[basisInds]\n",
    "    Phi_res = Phi[:,basisInds]\n",
    "    if printStuff:\n",
    "        print(\"basisInds:\")\n",
    "        print(basisInds)\n",
    "        print(\"alphas_res:\")\n",
    "        print(alphas_res.shape)\n",
    "    w_mode = np.zeros(len(alphas[basisInds]))\n",
    "    opt_res = minimize(fun=getLogPosterior, x0=w_mode.reshape((-1,1)), hess=getHessian, method='Newton-CG', \\\n",
    "                           args=(alphas_res, Phi_res, Phi_res.T, t), jac=True, options=opt_options)\n",
    "    \n",
    "    w_res = opt_res.x\n",
    "        \n",
    "    # X_res: features of the support vectors\n",
    "    # alphas_res: support vectors alphas\n",
    "    # w_res: optimal parameters' values\n",
    "    # Phi_res: Phi's of the support vectors\n",
    "    return X_res, alphas_res, w_res, Phi_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getLogPosterior(w, alphas, Phi, Phi_t, t):\n",
    "    # Return the objective's value at w\n",
    "    y = expit(np.dot(Phi, w))\n",
    "    res = 0\n",
    "    for i in range(len(t)):\n",
    "        if (t[i] == 1):\n",
    "            res += np.log(y[i])\n",
    "            continue\n",
    "        res += np.log(1-y[i])\n",
    "    diagAlphas = np.diag(alphas)\n",
    "    res -= 0.5 * np.dot(w.T, np.dot(diagAlphas, w)) #Decimal(0.5)*(np.dot(np.dot(w.T, A), w))\n",
    "    func_prime = np.dot(np.diag(alphas), w) - np.dot(Phi.T, (t-y))\n",
    "    # Invert for finding maximum\n",
    "    return -res, func_prime\n",
    "\n",
    "def getHessian(w, alphas, Phi, Phi_t, t):\n",
    "    y = expit(np.dot(Phi, w))\n",
    "    B = np.diag(y * (1 - y))\n",
    "    # Invert for finding maximum\n",
    "    return np.diag(alphas) + np.dot(Phi.T, np.dot(B, Phi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get prediction labels\n",
    "def predict(X_test, X_res, w_res, kernelType):\n",
    "    Phi = kernel(X_test, X_res, kernelType)\n",
    "    y = expit(np.dot(Phi, w_res))\n",
    "    y = np.column_stack((1-y, y))\n",
    "    res = np.full(y.shape[0], 0)\n",
    "    res[y[:, 1] > 0.5] = 1\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "start = time.clock()\n",
    "kernelTypes = [\"linear\", \"radial\", \"polynomial\", \"sigmoid\"]\n",
    "datasets = [\"RiplaySynthetic\", \"WAVEFORM\"]\n",
    "for dataset in datasets:\n",
    "    print(\"Dataset: \"+dataset+'\\n')\n",
    "    X_tr, Y_tr, X_ts, Y_ts = fetchDataset(dataset)\n",
    "    for kernelType in kernelTypes:\n",
    "        print(\"Kernel type: \"+kernelType)\n",
    "        X_res, alphas_res, w_res, Phi_res = sparse_RVM_C(X_tr, Y_tr, kernelType,update_fn = \"DD\", max_iter_num = 10000, opt_max_iter = 100)\n",
    "        trainingTime = time.clock() - start\n",
    "        print(\"Training time: \"+ str(trainingTime))\n",
    "\n",
    "        prediction = predict(X_ts, X_res, w_res, kernelType)\n",
    "        e_rvm_DD = np.sum(np.abs(prediction - Y_ts)) / X_ts.shape[0]\n",
    "        print(\"Prediction error: \"+ str(e_rvm_DD))\n",
    "        print(\"Number of relevance vectors: \"+str(X_res.shape[0])+'\\n')\n",
    "    print(\"-----------------------------------------------------------------\")\n",
    "\n",
    "plotClassification(X_tr, Y_tr, X_ts, prediction, X_res)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
