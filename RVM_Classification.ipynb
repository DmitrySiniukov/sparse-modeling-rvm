{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random as rnd\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import scipy.stats as sts\n",
    "import copy\n",
    "from decimal import *\n",
    "from sklearn.datasets import load_iris\n",
    "from scipy.optimize import minimize\n",
    "from scipy.special import expit\n",
    "from sklearn.base import BaseEstimator, RegressorMixin, ClassifierMixin\n",
    "from sklearn.metrics.pairwise import (linear_kernel, rbf_kernel, polynomial_kernel, sigmoid_kernel, laplacian_kernel)\n",
    "from sklearn.multiclass import OneVsOneClassifier\n",
    "from sklearn.utils.validation import check_X_y\n",
    "import data_gen\n",
    "import time\n",
    "dataDir = \"./Data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotData(x, t):\n",
    "    # Plot the data\n",
    "    classA = x[t == 1]\n",
    "    classB = x[t == 0]\n",
    "    plt.xlabel('X Axis (No units)')\n",
    "    plt.ylabel('Y Axis (No units)')\n",
    "    plt.plot(classA[:,0], classA[:,1], 'bo')\n",
    "    plt.plot(classB[:,0], classB[:,1], 'ro')\n",
    "    plt.legend([\"Class A\",\"Class B\"])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizeLabels(X, t):\n",
    "    result_X = []\n",
    "    result_t = []\n",
    "    classes = np.unique(t)\n",
    "    class0 = classes[0]\n",
    "    class1 = classes[1]\n",
    "    for i in range(len(t)):\n",
    "        if (t[i] == class0 or t[i] == class1):\n",
    "            if (t[i] == class0):\n",
    "                result_t.append(0)\n",
    "            else:\n",
    "                result_t.append(1)\n",
    "            result_X.append(X[i])\n",
    "    return np.array(result_X), np.array(result_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_tr: feature vectors of training dataset\n",
    "# t_tr: labels of training dataset\n",
    "# x_tst: grid of feature vectors for drawing classes' areas\n",
    "# t_tst: results of classification of the grid\n",
    "# x_sv: support vectors\n",
    "def plotClassification(x_tr, t_tr, x_tst, t_tst, x_sv):   \n",
    "    #First we plot the training points\n",
    "    classA_tr = x_tr[t_tr == 1]\n",
    "    classB_tr = x_tr[t_tr == 0]\n",
    "    \n",
    "    plt.clf()\n",
    "    plt.plot(classA_tr[:,0], classA_tr[:,1], 'bo')\n",
    "    plt.plot(classB_tr[:,0], classB_tr[:,1], 'ro')\n",
    "      \n",
    "    #Then, we plot the support vectors\n",
    "    plt.plot(x_sv[:,0], x_sv[:,1] , 'go', markersize = 15, mfc = \"None\") \n",
    "    \n",
    "    plt.legend([\"Class A\",\"Class B\", \"Support Vectors\"])\n",
    "    \n",
    "    #Finally, we plot the test results\n",
    "    classA_tst = x_tst[t_tst == 1]\n",
    "    classB_tst = x_tst[t_tst == 0]\n",
    "    \n",
    "    plt.xlabel('X Axis (No units)')\n",
    "    plt.ylabel('Y Axis (No units)')\n",
    "\n",
    "    plt.plot(classA_tst[:,0], classA_tst[:,1], 'bs', alpha=0.05)\n",
    "    plt.plot(classB_tst[:,0], classB_tst[:,1], 'rs', alpha=0.05)\n",
    "    #plt.savefig('toy_example_RVM_C.pdf')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def radialKernel(X1, X2, params):\n",
    "    print(\"Radial kernel parameter: \"+str(params[0]))\n",
    "    return laplacian_kernel(X1, X2, params[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linearKernel(X1, X2, params = None):\n",
    "    return linear_kernel(X1, X2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def polynomialKernel(X1, X2, params):\n",
    "    degree = params[0]\n",
    "    return polynomial_kernel(X1, X2, degree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoidKernel(X1, X2, params = None):\n",
    "    return sigmoid_kernel(X1, X2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussianKernel(X1, X2, params):\n",
    "    gamma = params[0]\n",
    "    return rbf_kernel(X1, X2, gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetchDataset(dataset):\n",
    "    return data_gen.genData(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X: training feature vectors\n",
    "# t: training labels\n",
    "# kernel: function of the form \"kernel(X1, X2, kernel_params)\"\n",
    "# kern_params: params for the passed kernel function\n",
    "# returns: \n",
    "def SSBL_RVM_C(X, t, kernel, kernelParams, update_fn = \"DD\", max_iter_num = 1000, opt_max_iter = 100, tolerance = 1e-5,\n",
    "          alpha_threshold = 1e6):\n",
    "\n",
    "    N = len(t)\n",
    "    Phi = kernel(X, X, kernelParams) #rbf_kernel(X, X, kern_param)\n",
    "    Phi = np.append(Phi, np.ones((N, 1)), axis = 1)\n",
    "    Phi_t = Phi.T\n",
    "    opt_options = { 'maxiter': opt_max_iter }\n",
    "    biasFlag = True\n",
    "    numLessTol = 0\n",
    "    w_mode = np.zeros(N+1)\n",
    "\n",
    "    # Initialize the variables\n",
    "    alphas = np.full(N+1, float(\"inf\"))\n",
    "    firstPhiCol = Phi[:,0]\n",
    "    norm = np.sum(firstPhiCol*firstPhiCol)\n",
    "    norm_t = np.sum(firstPhiCol*t)\n",
    "    # Calculate var[t], sigma^2\n",
    "    t_mean = np.mean(t)\n",
    "    sigma_sq = 0.1*np.sum(np.fabs(t-t_mean))/(N-1)\n",
    "    alphas[0] = (norm)/((norm_t/norm)-sigma_sq)\n",
    "\n",
    "    basisVars = np.full(N+1, False)\n",
    "    basisVars[0] = True\n",
    "    basisPhi = Phi[:,basisVars]\n",
    "\n",
    "    order = np.arange(N+1)\n",
    "    B = np.zeros((N,N))\n",
    "    B_inv = np.zeros((N,N))\n",
    "    t_y = np.zeros(N)\n",
    "    \n",
    "    for iterNum in range(max_iter_num):\n",
    "        basisChanged = False\n",
    "        alphasDiff = 0\n",
    "        #np.random.shuffle(order)\n",
    "        for ind in order:\n",
    "            basisAlphas = alphas[basisVars]\n",
    "            basisPhi = Phi[:,basisVars]\n",
    "            w0 = w_mode[basisVars]\n",
    "            #try\n",
    "            #    opt_res = minimize(fun=getLogPosterior, x0=np.zeros(len(basisAlphas)), hess=getHessian, method='Newton-CG', \\\n",
    "            #               args=(basisAlphas, basisPhi, basisPhi.T, t), jac=True, options=opt_options)\n",
    "            #except:\n",
    "            retryNum = 10\n",
    "            for i in range(1,retryNum):\n",
    "                try:\n",
    "                    opt_res = minimize(fun=getLogPosterior, x0=w0, hess=getHessian, method='Newton-CG', \\\n",
    "                        args=(basisAlphas, basisPhi, basisPhi.T, t), jac=True, options={ 'maxiter': opt_max_iter*i })\n",
    "                except:\n",
    "                    print(\"not converged\")\n",
    "                    w0 = np.zeros(len(basisAlphas))\n",
    "                if i == retryNum-1:\n",
    "                    continue\n",
    "            \n",
    "            counter = 0\n",
    "            for i in range(N+1):\n",
    "                if (not basisVars[i]):\n",
    "                    w_mode[i] = 0\n",
    "                else:\n",
    "                    w_mode[i] = opt_res.x[counter]\n",
    "                    counter += 1\n",
    "            \n",
    "            Sigma_inv = getHessian(w_mode, alphas, Phi, Phi_t, t)\n",
    "            Sigma_basis_inv = Sigma_inv[:,basisVars]\n",
    "            Sigma_basis_inv = Sigma_basis_inv[basisVars,:]\n",
    "            Sigma_basis = np.linalg.inv(Sigma_basis_inv)\n",
    "            \n",
    "            phi_w = np.dot(Phi, w_mode)\n",
    "            y = expit(phi_w)\n",
    "            # Compute B, B^{-1}\n",
    "            for i in range(N):\n",
    "                t_y[i] = t[i]-y[i]\n",
    "                B[i][i] = y[i]*(1-y[i])\n",
    "                if (B[i][i] == 0):\n",
    "                    B_inv[i][i] = float(\"inf\")\n",
    "                else:\n",
    "                    B_inv[i][i] = 1/B[i][i]\n",
    "            \n",
    "            phi_j = Phi[:,ind]\n",
    "            phi_B = np.dot(phi_j.T, B)\n",
    "            t_hat = phi_w + np.dot(B_inv,t_y)\n",
    "            long_product = np.dot(phi_B, basisPhi).dot(Sigma_basis).dot(basisPhi.T).dot(B)\n",
    "            S_j = np.dot(phi_B, phi_j) - np.dot(long_product, phi_j)\n",
    "            Q_j = np.dot(phi_B, t_hat) - np.dot(long_product, t_hat)\n",
    "            s_j = None\n",
    "            q_j = None\n",
    "            if (alphas[ind] > alpha_threshold):\n",
    "                s_j = S_j\n",
    "                q_j = Q_j\n",
    "            else:\n",
    "                s_j = (alphas[ind]*S_j)/(alphas[ind]-S_j)\n",
    "                q_j = (alphas[ind]*Q_j)/(alphas[ind]-S_j)\n",
    "                \n",
    "            if (q_j*q_j > s_j+tolerance):\n",
    "                if (alphas[ind] > alpha_threshold):\n",
    "                    basisVars[ind] = True\n",
    "                    basisChanged = True\n",
    "                alphaPrev = alphas[ind]\n",
    "                alphas[ind] = (s_j*s_j)/(q_j*q_j - s_j)\n",
    "                alphasDiff += np.fabs(alphas[ind]-alphaPrev)\n",
    "            else:\n",
    "                if (alphas[ind] < alpha_threshold):\n",
    "                    basisVars[ind] = False\n",
    "                    basisChanged = True\n",
    "                alphas[ind] = float(\"inf\")\n",
    "            \n",
    "        if ((not basisChanged) and alphasDiff < tolerance):\n",
    "            print(\"converged\")\n",
    "            break\n",
    "    \n",
    "    alphas_res = alphas[basisVars]\n",
    "    w_res = w_mode[basisVars]\n",
    "    Phi_res = Phi[:,basisVars]\n",
    "    X_res = X[basisVars[:-1],:]\n",
    "    \n",
    "    # X_res: features of the support vectors\n",
    "    # alphas_res: support vectors alphas\n",
    "    # w_res: optimal parameters' values\n",
    "    # Phi_res: Phi's of the support vectors\n",
    "    return X_res, alphas_res, w_res, Phi_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function filters out infinite alpha values and the columns of the design\n",
    "# matrix that correspond to them\n",
    "def filterOutInf(alphaVec,designMatrix,threshHold):\n",
    "    nonInfIdx = np.where(alphaVec < threshHold)[0]\n",
    "    filteredAlphaVec = alphaVec[nonInfIdx]\n",
    "    filteredDesignMatrix = designMatrix[:,nonInfIdx]\n",
    "    return filteredAlphaVec, filteredDesignMatrix\n",
    "\n",
    "def sigmoidFn(y):\n",
    "    return 1/(1+np.exp(-y))\n",
    "\n",
    "def getqiAndsi(OGPhi,Phi,Phi_t, alphaVec,t,w,Sigma,threshold,basisInds,index):\n",
    "    phiI = OGPhi[:,index].reshape((-1,1))\n",
    "    phiI_t =phiI.T\n",
    "    alphaI = alphaVec[index]\n",
    "    y = np.array([np.dot(w.T,xn[basisInds]) for xn in OGPhi]).reshape((-1,1))\n",
    "    t = t.reshape((-1,1))\n",
    "    betas = np.array([sigmoidFn(yn)*(1-sigmoidFn(yn)) for yn in y]).flatten()\n",
    "    B = np.diag(betas)\n",
    "    if B.ndim == 1:\n",
    "        B = B.reshape((-1,1))\n",
    "    tEst = np.dot(Phi,w) + np.linalg.inv(B).dot((np.subtract(t,y)))\n",
    "\n",
    "    Qi = phiI_t.dot(B).dot(tEst)\n",
    "    Qi -= phiI_t.dot(B).dot(Phi).dot(Sigma).dot(Phi_t).dot(B).dot(tEst)\n",
    "\n",
    "    Si = phiI_t.dot(B).dot(phiI)\n",
    "    Si -= phiI_t.dot(B).dot(Phi).dot(Sigma).dot(Phi_t).dot(B).dot(phiI)\n",
    "\n",
    "    if alphaI >= threshold:\n",
    "        qi = Qi\n",
    "        si = Si\n",
    "    else:\n",
    "        qi = alphaI*Qi/(alphaI-Si)\n",
    "        si = alphaI*Si/(alphaI-Si)\n",
    "\n",
    "    return qi, si\n",
    "\n",
    "def getC(alphaVec,designMatrix,designMatrix_t,postMean):\n",
    "    y = expit(np.dot(designMatrix, postMean))\n",
    "    B = np.diag(y * (1 - y))\n",
    "    A = np.diag(alphaVec)\n",
    "    return B + np.dot(np.dot(designMatrix,A),designMatrix_t)\n",
    "    \n",
    "def updateAlphaValue(designMatrix,alphaVec,postMean,t,postCov,threshHold, index):\n",
    "    C = getC(alphaVec,designMatrix,postMean)\n",
    "    qi, si = getqiAndsi(designMatrix, alphaVec, C, t,postCov, threshHold,index)\n",
    "    if qi**2 > si:\n",
    "        return si**2/(qi**2-si)\n",
    "    else:\n",
    "        return math.inf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X: training feature vectors\n",
    "# t: training labels\n",
    "# kernel: function of the form \"kernel(X1, X2, kernel_params)\"\n",
    "# kern_params: params for the passed kernel function\n",
    "# returns: \n",
    "def RVM_C(X, t, kernel, kernelParams, update_fn = \"DD\", max_iter_num = 1000, opt_max_iter = 100, tolerance = 1e-7, alpha_threshold = 1e7):\n",
    "\n",
    "    # Gamma prior variables:\n",
    "    a = 0\n",
    "    b = 0\n",
    "    c = 0\n",
    "    d = 0 # These are all set to 0 since we, just like Tipping, assume uniform priors for the hyperparams\n",
    "    \n",
    "    N = len(t)\n",
    "    Phi = kernel(X, X, kernelParams) #rbf_kernel(X, X, kern_param)\n",
    "    Phi = np.append(Phi, np.ones((N, 1)), axis = 1)\n",
    "    Phi_t = Phi.T\n",
    "    opt_options = { 'maxiter': opt_max_iter }\n",
    "    biasFlag = True\n",
    "    numLessTol = 0\n",
    "\n",
    "    numRelevant = N+1\n",
    "    alphas = np.full(numRelevant, 1e-6)\n",
    "    w_mode = np.zeros(numRelevant)\n",
    "\n",
    "    alphas_res = None\n",
    "    Phi_res = None\n",
    "    w_res = None\n",
    "    X_res = copy.deepcopy(X)\n",
    "\n",
    "    for iterNum in range(max_iter_num):\n",
    "        opt_res = minimize(fun=getLogPosterior, x0=w_mode, hess=getHessian, method='Newton-CG', \\\n",
    "                           args=(alphas, Phi, Phi_t, t), jac=True, options=opt_options)\n",
    "\n",
    "        w_mode = opt_res.x\n",
    "        Sigma = np.linalg.inv(getHessian(w_mode, alphas, Phi, Phi_t, t))\n",
    "\n",
    "        # Prune basis functions\n",
    "\n",
    "        basisInds = []\n",
    "        sumDiff = 0\n",
    "        for i in range(numRelevant):\n",
    "            alpha_prev = alphas[i]\n",
    "            \n",
    "            if update_fn == \"DD\":\n",
    "                alphas[i] = (1 - Sigma[i][i]*alpha_prev)/(w_mode[i]*w_mode[i])\n",
    "            elif update_fn == \"EM\":\n",
    "                weightEst = Sigma[i,i] + w_mode[i]**2\n",
    "                alphas[i] = (1+2*a)/(weightEst+2*b)\n",
    "                \n",
    "            if (alphas[i] < alpha_threshold):\n",
    "                basisInds.append(i)\n",
    "                sumDiff += np.fabs(alphas[i]-alpha_prev)\n",
    "\n",
    "        if (sumDiff < tolerance):\n",
    "            numLessTol += 1\n",
    "        if (numLessTol > 3 or iterNum == max_iter_num-1):\n",
    "            # Save results\n",
    "            \n",
    "            numRelevant = len(basisInds)\n",
    "\n",
    "            alphas_res = np.zeros(numRelevant)\n",
    "            Phi_res = np.zeros((N, numRelevant))\n",
    "            w_res = np.zeros(numRelevant)\n",
    "            X_new = np.zeros((numRelevant - (1 if biasFlag else 0), len(X[0])))\n",
    "            # a if condition else b\n",
    "\n",
    "            for i in range(numRelevant):\n",
    "                alphas_res[i] = alphas[basisInds[i]]\n",
    "                w_res[i] = w_mode[basisInds[i]]\n",
    "                for j in range(N):\n",
    "                    Phi_res[j][i] = Phi[j][basisInds[i]]\n",
    "                if (basisInds[i] != len(X_res) and i != len(X_new)):\n",
    "                    for j in range(len(X[0])):\n",
    "                        X_new[i][j] = X_res[basisInds[i]][j]\n",
    "\n",
    "            X_res = X_new\n",
    "\n",
    "            break\n",
    "\n",
    "        if (len(basisInds) == 0):\n",
    "            basisInds.append(0)\n",
    "            # First iteration\n",
    "            if (biasFlag):\n",
    "                basisInds.append(len(alphas)-1)\n",
    "\n",
    "        if (biasFlag):\n",
    "            if not (alphas[-1] < alpha_threshold):\n",
    "                biasFlag = False\n",
    "\n",
    "        numRelevant = len(basisInds)\n",
    "\n",
    "        alphas_new = np.zeros(numRelevant)\n",
    "        Phi_new = np.zeros((N, numRelevant))\n",
    "        w_mode_new = np.zeros(numRelevant)\n",
    "        X_new = np.zeros((numRelevant - (1 if biasFlag else 0), len(X[0])))\n",
    "\n",
    "        for i in range(numRelevant):\n",
    "            alphas_new[i] = alphas[basisInds[i]]\n",
    "            w_mode_new[i] = w_mode[basisInds[i]]\n",
    "            for j in range(N):\n",
    "                Phi_new[j][i] = Phi[j][basisInds[i]]\n",
    "            if (basisInds[i] != len(X_res) and i != len(X_new)):\n",
    "                for j in range(len(X[0])):\n",
    "                    X_new[i][j] = X_res[basisInds[i]][j]\n",
    "\n",
    "        alphas = alphas_new\n",
    "        w_mode = w_mode_new\n",
    "        Phi = Phi_new\n",
    "        Phi_t = Phi.T\n",
    "        X_res = X_new\n",
    "        \n",
    "    # X_res: features of the support vectors\n",
    "    # alphas_res: support vectors alphas\n",
    "    # w_res: optimal parameters' values\n",
    "    # Phi_res: Phi's of the support vectors\n",
    "    return X_res, alphas_res, w_res, Phi_res\n",
    "\n",
    "def sparse_RVM_C(X, t, kernel, kernelParams, update_fn = \"DD\", max_iter_num = 10000, opt_max_iter = 100, tolerance = 1e-7,\n",
    "                 alpha_threshold = 1e5):\n",
    "\n",
    "    N = len(t)\n",
    "    Phi = kernel(X, X, kernelParams)\n",
    "    Phi = np.append(Phi, np.ones((N, 1)), axis = 1)\n",
    "    Phi_t = Phi.T\n",
    "    opt_options = { 'maxiter': opt_max_iter }\n",
    "    biasFlag = True\n",
    "    numLessTol = 0\n",
    "\n",
    "    numRelevant = 1\n",
    "    alphas = np.full(N+1, float('inf'))\n",
    "    alphas[0] = 1e-6\n",
    "\n",
    "    alphas_res = None\n",
    "    Phi_res = None\n",
    "    w_res = None\n",
    "    X_res = copy.deepcopy(X)\n",
    "    usedPhi = Phi[:,0].reshape((-1,1))\n",
    "    basisInds =[0]\n",
    "    w_mode = np.zeros(len(alphas[basisInds]))\n",
    "    printStuff = False\n",
    "\n",
    "    for iterNum in range(max_iter_num):\n",
    "        \n",
    "        if printStuff:\n",
    "            print(\"New iteration\")\n",
    "            print(\"Used Phi:\")\n",
    "            print(usedPhi.shape)\n",
    "            print(\"w_mode:\")\n",
    "            print(w_mode.shape)\n",
    "            print(\"alphas:\")\n",
    "            print(alphas.shape)\n",
    "            print(\"basisInds:\")\n",
    "            print(basisInds)\n",
    "        w_mode = np.zeros(len(alphas[basisInds]))\n",
    "        usedPhi = Phi[:,basisInds]\n",
    "        opt_res = minimize(fun=getLogPosterior, x0=w_mode.reshape((-1,1)), hess=getHessian, method='Newton-CG', \\\n",
    "                           args=(alphas[basisInds], usedPhi, usedPhi.T, t), jac=True, options=opt_options)\n",
    "\n",
    "        w_mode = opt_res.x\n",
    "        Sigma = np.linalg.inv(getHessian(w_mode, alphas[basisInds], usedPhi, usedPhi.T, t))\n",
    "        sVec = []\n",
    "        qVec = []\n",
    "        \n",
    "        usedPhi = Phi[:,basisInds]\n",
    "        usedPhi_t = usedPhi.T\n",
    "        for i in range(len(alphas)):\n",
    "            qi, si = getqiAndsi(Phi,usedPhi,usedPhi_t, alphas,t,w_mode.reshape((-1,1)),Sigma,alpha_threshold,basisInds,i)\n",
    "            qVec.append(qi)\n",
    "            sVec.append(si)\n",
    "        \n",
    "        # Prune basis functions\n",
    "\n",
    "        sumDiff = 0\n",
    "        for i in range(len(alphas)):\n",
    "            alpha_prev = alphas[i]\n",
    "            \n",
    "            if update_fn == \"DD\":\n",
    "                if qVec[i]**2 > sVec[i]:\n",
    "                    if alphas[i] > alpha_threshold:\n",
    "                        basisInds.append(i)\n",
    "                        numRelevant += 1\n",
    "                    \n",
    "                    alphas[i] = sVec[i]**2/(qVec[i]**2-sVec[i])\n",
    "                elif qVec[i]**2 <= sVec[i] and i in basisInds:\n",
    "                    alphas[i] = float('inf')\n",
    "                    basisInds.remove(i)\n",
    "                    numRelevant -= 1\n",
    "                    \n",
    "            elif update_fn == \"EM\":\n",
    "                weightEst = Sigma[i,i] + w_mode[i]**2\n",
    "                alphas[i] = (1+2*a)/(weightEst+2*b)\n",
    "                \n",
    "            if (alphas[i] < alpha_threshold):\n",
    "                #basisInds.append(i)\n",
    "                sumDiff += np.fabs(alphas[i]-alpha_prev)\n",
    "\n",
    "        if (sumDiff < tolerance):\n",
    "            numLessTol += 1\n",
    "        if (numLessTol > 3):\n",
    "            # Save results\n",
    "\n",
    "            break\n",
    "\n",
    "        if (len(basisInds) == 0):\n",
    "            basisInds.append(0)\n",
    "            # First iteration\n",
    "            if (biasFlag):\n",
    "                basisInds.append(len(alphas)-1)\n",
    "\n",
    "        if (biasFlag):\n",
    "            if not (alphas[-1] < alpha_threshold):\n",
    "                biasFlag = False\n",
    "                \n",
    "    \n",
    "    alphas_res = alphas[basisInds]\n",
    "    X_res = X[basisInds]\n",
    "    Phi_res = Phi[:,basisInds]\n",
    "    if printStuff:\n",
    "        print(\"basisInds:\")\n",
    "        print(basisInds)\n",
    "        print(\"alphas_res:\")\n",
    "        print(alphas_res.shape)\n",
    "    w_mode = np.zeros(len(alphas[basisInds]))\n",
    "    opt_res = minimize(fun=getLogPosterior, x0=w_mode.reshape((-1,1)), hess=getHessian, method='Newton-CG', \\\n",
    "                           args=(alphas_res, Phi_res, Phi_res.T, t), jac=True, options=opt_options)\n",
    "    \n",
    "    w_res = opt_res.x\n",
    "        \n",
    "    # X_res: features of the support vectors\n",
    "    # alphas_res: support vectors alphas\n",
    "    # w_res: optimal parameters' values\n",
    "    # Phi_res: Phi's of the support vectors\n",
    "    return X_res, alphas_res, w_res, Phi_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getLogPosterior(w, alphas, Phi, Phi_t, t):\n",
    "    # Return the objective's value at w\n",
    "    y = expit(np.dot(Phi, w))\n",
    "    res = 0\n",
    "    for i in range(len(t)):\n",
    "        if (t[i] == 1):\n",
    "            res += np.log(y[i])\n",
    "            continue\n",
    "        res += np.log(1-y[i])\n",
    "    diagAlphas = np.diag(alphas)\n",
    "    res -= 0.5 * np.dot(w.T, np.dot(diagAlphas, w))\n",
    "    func_prime = np.dot(np.diag(alphas), w) - np.dot(Phi.T, (t-y))\n",
    "    # Invert for finding maximum\n",
    "    return -res, func_prime\n",
    "\n",
    "def getHessian(w, alphas, Phi, Phi_t, t):\n",
    "    y = expit(np.dot(Phi, w))\n",
    "    B = np.diag(y * (1 - y))\n",
    "    # Invert for finding maximum\n",
    "    return np.diag(alphas) + np.dot(Phi.T, np.dot(B, Phi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get prediction labels\n",
    "def predict(X_test, X_res, w_res, kernel, kernelParams):\n",
    "    Phi = kernel(X_test, X_res, kernelParams)\n",
    "    if (np.shape(Phi)[1] != np.shape(w_res)[0]):\n",
    "        Phi = np.append(Phi, np.ones((np.shape(Phi)[0], 1)), axis = 1)\n",
    "    y = expit(np.dot(Phi, w_res))\n",
    "    y = np.column_stack((1-y, y))\n",
    "    res = np.full(y.shape[0], 0)\n",
    "    res[y[:, 1] > 0.5] = 1\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def genRipley():\n",
    "    eps = 1e-30\n",
    "    # We load the Ripley mixture model dataset\n",
    "    file = dataDir + 'ripley.txt'\n",
    "    data = np.loadtxt(file, skiprows=1)\n",
    "    X = data[:,0:2]\n",
    "    Y = data[:,2]\n",
    "    # We put the 0-class to -1-class to adjust to our notation\n",
    "    Y[Y == 0] = -1\n",
    "    # We finally select only 100 points (to follow Tipping paper)\n",
    "    index1 = rnd.sample(range(0, 499), 50)\n",
    "    index2 = rnd.sample(range(500, 1000), 50)\n",
    "    index = np.hstack((index1, index2))\n",
    "    X = X[index]\n",
    "    Y = Y[index]\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: RiplaySynthetic\n",
      "\n",
      "Kernel type: radial\n",
      "Radial kernel parameter: 3.5\n",
      "converged\n",
      "Training time: 877.765548\n",
      "Radial kernel parameter: 3.5\n",
      "Prediction error: 0.118\n",
      "Number of relevance vectors: 12\n",
      "\n",
      "-----------------------------------------------------------------\n",
      "Dataset: WAVEFORM\n",
      "\n",
      "Kernel type: radial\n",
      "Radial kernel parameter: 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/site-packages/ipykernel_launcher.py:7: RuntimeWarning: divide by zero encountered in log\n",
      "  import sys\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not converged\n",
      "not converged\n",
      "not converged\n",
      "not converged\n",
      "not converged\n",
      "not converged\n",
      "not converged\n",
      "not converged\n",
      "not converged\n",
      "converged\n",
      "Training time: 1773.547405\n",
      "Radial kernel parameter: 0.5\n",
      "Prediction error: 0.183333333333\n",
      "Number of relevance vectors: 130\n",
      "\n",
      "-----------------------------------------------------------------\n",
      "Dataset: Banana\n",
      "\n",
      "Kernel type: radial\n",
      "Radial kernel parameter: 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/site-packages/ipykernel_launcher.py:9: RuntimeWarning: divide by zero encountered in log\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converged\n",
      "Training time: 3762.894897\n",
      "Radial kernel parameter: 0.5\n",
      "Prediction error: 0.271666666667\n",
      "Number of relevance vectors: 27\n",
      "\n",
      "-----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "start = time.clock()\n",
    "kernels = [radialKernel]\n",
    "kernelParams = [[7.5]]\n",
    "kernelTitles = [\"radial\"]\n",
    "datasets = [\"RiplaySynthetic\", \"WAVEFORM\",\"Banana\"]\n",
    "for dataset in datasets:\n",
    "    if dataset == \"RiplaySynthetic\":\n",
    "        kernelParams[0] = [3.5]\n",
    "    else:\n",
    "        kernelParams[0] = [0.5]\n",
    "    print(\"Dataset: \"+dataset+'\\n')\n",
    "    X_tr, Y_tr, X_ts, Y_ts = fetchDataset(dataset)\n",
    "    for i in range(len(kernels)):\n",
    "        kernel = kernels[i]\n",
    "        kernelPars = kernelParams[i]\n",
    "        print(\"Kernel type: \" + kernelTitles[i])\n",
    "        X_res, alphas_res, w_res, Phi_res = SSBL_RVM_C(X_tr, Y_tr, kernel, kernelPars, update_fn = \"DD\", \\\n",
    "                                                  max_iter_num = 3000, opt_max_iter = 100)\n",
    "        trainingTime = time.clock() - start\n",
    "        print(\"Training time: \"+ str(trainingTime))\n",
    "\n",
    "        prediction = predict(X_ts, X_res, w_res, kernel, kernelPars)\n",
    "        e_rvm_DD = np.sum(np.abs(prediction - Y_ts)) / X_ts.shape[0]\n",
    "        print(\"Prediction error: \"+ str(e_rvm_DD))\n",
    "        print(\"Number of relevance vectors: \"+str(X_res.shape[0])+'\\n')\n",
    "    print(\"-----------------------------------------------------------------\")\n",
    "\n",
    "#plotClassification(X_tr, Y_tr, X_ts, prediction, X_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ripley toy example\n",
    "X_tr, t_tr = genRipley()\n",
    "X_tr, t_tr = normalizeLabels(X_tr, t_tr)\n",
    "h = 70\n",
    "x_0_min = np.min(X_tr[:,0])\n",
    "x_0_max = np.max(X_tr[:,0])\n",
    "x_1_min = np.min(X_tr[:,1])\n",
    "x_1_max = np.max(X_tr[:,1])\n",
    "xx0, xx1 = np.meshgrid(np.linspace(x_0_min, x_0_max, num=h), np.linspace(x_1_min, x_1_max, num=h))\n",
    "X_tst = np.vstack((xx0.reshape(1,-1),xx1.reshape(1,-1))).T\n",
    "#plotData(X_tr, t_tr)\n",
    "kernel = gaussianKernel\n",
    "kernelParams = [0.5]\n",
    "X_res, alphas_res, w_res, Phi_res = SSBL_RVM_C(X_tr, t_tr, kernel, kernelParams, update_fn = \"DD\", max_iter_num = 1000, \\\n",
    "                                               opt_max_iter = 300)\n",
    "prediction = predict(X_tst, X_res, w_res, kernel, kernelParams)\n",
    "plotClassification(X_tr, t_tr, X_tst, prediction, X_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iris toy example\n",
    "iris = load_iris()\n",
    "X = []\n",
    "t = []\n",
    "\n",
    "for i in range(len(iris.data)):\n",
    "    if iris.target[i] == 1 or iris.target[i] == 2:\n",
    "        X.append(iris.data[i][:-2])\n",
    "        t.append((iris.target[i]-1)) # TODO: Identify unique labels, change to 0/1-format\n",
    "X = np.array(X)\n",
    "t = np.array(t)\n",
    "\n",
    "h = 55\n",
    "x_0_min = np.min(X[:,0])\n",
    "x_0_max = np.max(X[:,0])\n",
    "x_1_min = np.min(X[:,1])\n",
    "x_1_max = np.max(X[:,1])\n",
    "xx0, xx1 = np.meshgrid(np.linspace(x_0_min, x_0_max, num=h),\n",
    "                         np.linspace(x_1_min, x_1_max, num=h))\n",
    "X_tst = np.vstack((xx0.reshape(1,-1),xx1.reshape(1,-1))).T\n",
    "\n",
    "kernel = radialKernel\n",
    "kernelParams = [7.5]\n",
    "X_res, alphas_res, w_res, Phi_res = SSBL_RVM_C(X, t, kernel, kernelParams, update_fn = \"DD\", max_iter_num = 1000, opt_max_iter = 100)\n",
    "prediction = predict(X_tst, X_res, w_res, kernel, kernelParams)\n",
    "plotClassification(X, t, X_tst, prediction, X_res)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
